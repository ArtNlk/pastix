extern "C" %{
/**
 *
 * @file zgetrf_sp1dplus.jdf
 *
 * PaRSEC 1D algorithm jdf for LU factorization.
 *
 * @copyright 2016-2017 Bordeaux INP, CNRS (LaBRI UMR 5800), Inria,
 *                      Univ. Bordeaux. All rights reserved.
 *
 * @version 6.0.0
 * @author Mathieu Faverge
 * @date 2013-06-24
 * @precisions normal z -> s d c
 *
 **/
#include <parsec.h>
#include <parsec/data_distribution.h>
#include <parsec/private_mempool.h>
#include "common.h"
#include "solver.h"
#include "pastix_zcores.h"
#include "sopalin_data.h"
#include "pastix_parsec_gpu.h"

%}

/* Globals
 */
descA        [type = "parsec_sparse_matrix_desc_t *" ]
sopalin_data [type = "sopalin_data_t *" ]

forced_pushout [type = "int" hidden = on default = "(0)" ]
datacode  [type = "SolverMatrix*"         hidden = on default = "(sopalin_data->solvmtx)"       ]
cblknbr   [type = "pastix_int_t"          hidden = on default = "(datacode->cblknbr - 1)"       ]
bloknbr   [type = "pastix_int_t"          hidden = on default = "(datacode->bloknbr - 2)"       ]
lowrank   [type = "pastix_lr_t"           hidden = on default = "(sopalin_data->solvmtx->lowrank)"]

p_work    [type = "parsec_memory_pool_t *"]
lwork     [type = "pastix_int_t"]

cpu_coefs [ type = "pastix_fixdbl_t *" hidden = on default = "&((*(sopalin_data->cpu_coefs))[PastixKernelGEMMCblk2d2d][0])" ]
gpu_coefs [ type = "pastix_fixdbl_t *" hidden = on default = "&((*(sopalin_data->cpu_coefs))[PastixKernelGEMMCblk2d2d][0])" ]

/**************************************************
 *                   GETRF                        *
 * panel factorization: do trf of diagonal and    *
 *                    : trsm on off-diagonal      *
 **************************************************/
GETRF(k) [high_priority = on]

// Execution space
k = 0 .. cblknbr

browk0    = %{ SolverCblk *cblk = datacode->cblktab + k;     return cblk->brownum; %}
browk1    = %{ SolverCblk *cblk = datacode->cblktab + k + 1; return cblk->brownum; %}
lastbrow  = %{ if ( browk0 == browk1 ) return 0; else return datacode->browtab[ browk1 - 1 ]; %}
firstblok = %{ SolverCblk *cblk = datacode->cblktab + k;     return cblk->fblokptr - datacode->bloktab + 1; %}
lastblok  = %{ SolverCblk *cblk = datacode->cblktab + k + 1; return cblk->fblokptr - datacode->bloktab - 1; %}

// Parallel partitioning
:descA(0, k, 0)

// Parameters
/* C is A(k) if it's a leaf or get the cblk from the last update */
RW L <- ( browk0 == browk1 ) ? descA(0, k, 0) : C GEMM1D( 0, lastbrow )
     -> A GEMM1D(0 .. 1, firstblok .. lastblok)
     -> descA(0, k, 0)

RW U <- ( browk0 == browk1 ) ? descA(1, k, 0) : C GEMM1D( 1, lastbrow )
     -> B GEMM1D(0 .. 1, firstblok .. lastblok)
     -> descA(1, k, 0)

; %{ return cblknbr - k; %}

BODY
{
    SolverCblk *cblk = datacode->cblktab + k;
    if (!(cblk->cblktype & CBLK_IN_SCHUR))
        cpucblk_zgetrfsp1d_panel(cblk, L, U, sopalin_data->diagthreshold, &lowrank );
}
END

/**
 *       GEMM
 *
 * To have a contiguous range of GEMM to release in the potrf, they are numbered
 * with the indexes of the off-diagonal blocks, diagonal block included.
 * Thus, the diagonal block tasks which doesn't perfom computations are used as
 * DATA_IN tasks. This is mandatory when using the GPU, due to the versioning
 * bumped by the cpu version of the diagonal block that coccurs when computing
 * the diagonal blocks.
 *
 * For all off-diagonal blocks, it updates the trailing matrix with the panel
 * k-th block updating corresponding.
 *
 */
GEMM1D(s, bloknum)

// Execution space
s       = 0 .. 1
bloknum = 1 .. bloknbr

lcblknm = %{ SolverBlok *blok = datacode->bloktab + bloknum;     return blok->lcblknm; %}
fcblknm = %{ SolverBlok *blok = datacode->bloktab + bloknum;     return blok->fcblknm; %}
first   = %{ SolverCblk *cblk = datacode->cblktab + fcblknm;     return cblk->brownum; %}
last    = %{ SolverCblk *cblk = datacode->cblktab + fcblknm + 1; return cblk->brownum - 1; %}

brownum = %{ return datacode->bloktab[bloknum].browind; %} /* -1 if diagonal block */
prev    = %{
    assert( first >= 0 );
    /**
     * If bloknum is a diagonal block, or if it is the first one applied on C,
     * there is no previous
     */
    if ((brownum == -1) || (brownum == first) ) {
        return 0;
    }
    /**
     * Otherwise we return the previous block in the list of blocks facing fcblk
     */
    else {
        assert( brownum > first );
        return datacode->browtab[brownum-1];
    }
    %}
next    = %{
    /**
     * If we are on a diagonal blok, or if we are the last one, there is no next
     */
    if ((brownum == -1) || (brownum >= last) ){
        return 0;
    } else {
        return datacode->browtab[brownum+1];
    }
    %}

n  = %{ if (brownum == -1) return 0; else return blok_rownbr(datacode->bloktab + bloknum); %}
k  = %{ if (brownum == -1) return 0; else return cblk_colnbr(datacode->cblktab + lcblknm); %}
m  = %{ if (brownum == -1) return 0; else {
        if ((datacode->cblktab + lcblknm)->cblktype & CBLK_LAYOUT_2D) {
            return datacode->cblktab[lcblknm].stride - (datacode->bloktab[bloknum].coefind / k);
        } else {
            return datacode->cblktab[lcblknm].stride - datacode->bloktab[bloknum].coefind;
        }
    }%}

// Parallel partitioning
:descA(s, fcblknm, 0)

// Parameters
READ  A  <- (brownum != -1) ? L GETRF( lcblknm ) : NULL
READ  B  <- (brownum != -1) ? U GETRF( lcblknm ) : NULL

RW    C  <- (brownum == -1) ? NULL
         <- (brownum != -1) && (brownum == first) ? descA( s, fcblknm, 0 )
         <- (brownum != -1) && (brownum != first) ? C GEMM1D( s, prev )

         -> (brownum != -1) && (brownum == last) && (s == 0) ? L GETRF( fcblknm )
         -> (brownum != -1) && (brownum == last) && (s != 0) ? U GETRF( fcblknm )
         -> (brownum != -1) && (brownum != last)             ? C GEMM1D( s, next )

; %{ return cblknbr - ((fcblknm + lcblknm) / 2 ) + last - brownum; %}

BODY [ type=CUDA
       pushout=forced_pushout
       device="((brownum == -1) ? -2 : -1)"
       weight="(last-brownum+1)"
       cpu_cost="modelsGetCost3Param( cpu_coefs, m, n, k )"
       gpu_cost="modelsGetCost3Param( gpu_coefs, m, n, k )"
       /* devicefct="pastix_parsec_selectgpu_gemm1d_3p_fct( this_task, ratio, modelsGetCost3Param( cpu_coefs, m, n, k ), modelsGetCost3Param( gpu_coefs, m, n, k ), brownum )" */
       /* device=%{ if (brownum == -1) return (datacode->cblktab + fcblknm)->gpuid; else { return parsec_zgemm1d_selectgpu( datacode, bloknum, last - first + 1 ); } %} */
       /* devicefct=zgetrf_sp1dplus_zgemm1d_selectgpu_fct */
]
#if defined(PASTIX_WITH_CUDA)
{
    /* Never execute the GPU kernel on diagonal blocks */
    if (brownum != -1) {
        SolverCblk *lcblk = datacode->cblktab + lcblknm;
        SolverCblk *fcblk = datacode->cblktab + fcblknm;
        SolverBlok *blok  = datacode->bloktab + bloknum;

        if (!(lcblk->cblktype & CBLK_IN_SCHUR)) {
            if ( blok+s < lcblk[1].fblokptr ) {
                if ( s == 0 ) {
#if defined(PASTIX_CUDA_FERMI)
                    gpu_zgemmsp_fermi( datacode,
                                       PastixLower, PastixTrans,
                                       descA->d_blocktab[parsec_body.index],
                                       lcblk, blok, fcblk,
                                       A, B, C,
                                       parsec_body.stream );
#else
                    gpucblk_zgemmsp( PastixLCoef, PastixUCoef, PastixTrans,
                                     lcblk, blok, fcblk,
                                     A, B, C,
                                     &lowrank, parsec_body.stream );
#endif /* defined(PASTIX_CUDA_FERMI) */
                }
                else {
#if defined(PASTIX_CUDA_FERMI)
                    gpu_zgemmsp_fermi( datacode,
                                       PastixUpper, PastixTrans,
                                       descA->d_blocktab[parsec_body.index],
                                       lcblk, blok, fcblk,
                                       B, A, C,
                                       parsec_body.stream );
#else
                    gpucblk_zgemmsp( PastixUCoef, PastixLCoef, PastixTrans,
                                     lcblk, blok, fcblk,
                                     B, A, C,
                                     &lowrank, parsec_body.stream );
#endif /* defined(PASTIX_CUDA_FERMI) */
                }
            }
        }
    }
}
#endif
END

BODY
{
    /* If diagonal block, we skip it */
    if (brownum != -1) {
        SolverCblk *lcblk = datacode->cblktab + lcblknm;
        SolverCblk *fcblk = datacode->cblktab + fcblknm;
        SolverBlok *blok  = datacode->bloktab + bloknum;
        pastix_complex64_t *work = NULL;

        if (!(lcblk->cblktype & CBLK_IN_SCHUR)) {
            /* Update */
            if ( blok+s < lcblk[1].fblokptr ) {
                if ( lwork > 0 ) {
                    work = (pastix_complex64_t *)parsec_private_memory_pop( p_work );
                }

                if (s == 0) {
                    cpucblk_zgemmsp( PastixLCoef, PastixUCoef, PastixTrans,
                                     lcblk, blok, fcblk,
                                     A, B, C,
                                     work, lwork, &lowrank );
                }
                else {
                    cpucblk_zgemmsp( PastixUCoef, PastixLCoef, PastixTrans,
                                     lcblk, blok, fcblk,
                                     B, A, C,
                                     work, lwork, &lowrank );
                }

                if ( work  ) {
                    parsec_private_memory_push( p_work, (void *)work );
                }
            }
        }
    }
}
END

extern "C" %{
#if defined(PARSEC_GPU_WITH_CUDA)
#if defined(PASTIX_WITH_CUDA) && 0
int
zgetrf_sp1dplus_zgemm1d_selectgpu_fct( const __parsec_zgetrf_sp1dplus_GEMM1D_task_t *this_task,
                                       double weight )
{
    pastix_int_t dev_index, gpuid = -2;
    const int brownum = this_task->locals.brownum.value;

    if (brownum == -1) {
      return -2;
    }

    /**
     * Look if one of the GPU owns the C data
     */
    dev_index = this_task->data._f_C.data_in->original->owner_device;
    if (dev_index > 1) {
        gpuid = dev_index - 2;
    }

    /**
     * If not already on the GPU, find the best one
     */
    if ( gpuid == -2 ) {
        __parsec_zgetrf_sp1dplus_internal_handle_t *__parsec_handle = (__parsec_zgetrf_sp1dplus_internal_handle_t *)this_task->parsec_handle;
        const __parsec_zgetrf_sp1dplus_GEMM1D_assignment_t *assignments = &(this_task->locals);
        const int bloknum = assignments->bloknum.value;
        const int lcblknm = assignments->lcblknm.value;
        const int fcblknm = assignments->fcblknm.value;
        SolverMatrix *datacode = __parsec_handle->super._g_datacode;
        SolverCblk *cblk  = (datacode)->cblktab + lcblknm;
        SolverCblk *fcblk = (datacode)->cblktab + fcblknm;
        SolverBlok *blok  = (datacode)->bloktab + bloknum;
        pastix_int_t M, N, K, lda, ldc;
        double gpu_cost, mintime, transfer_cost;
        int dev, nbdevices;
        double cost;

        K = cblk_colnbr( cblk );
        M = cblk->stride;
        M-= (cblk->cblktype & CBLK_LAYOUT_2D) ? blok->coefind / K : blok->coefind;
        N = blok_rownbr( blok );
        lda = cblk->stride;
        ldc = fcblk->stride;

        mintime = cost_gemm( M, N, K, 0 ) / 1.e9; /* CPU_COST in s. */
        mintime = mintime > 0. ? mintime : 0.;

        gpu_cost = cost_gemm( M, N, K, 1 ) / 1.e9;
        gpu_cost = gpu_cost > 0. ? gpu_cost : 0.;

        transfer_cost = sizeof(pastix_complex64_t) / 1.e9 / bandwidth;

        nbdevices = parsec_devices_enabled()-2;
        for( dev = 0; dev < nbdevices; dev++ ) {
            cost = ( 1 + parsec_nbtasks_on_gpu[dev] ) * gpu_cost + transfer_cost;

            /* Compute the transfer cost of A */
            if ( (weight < 5) && (this_task->data._f_Al.data_in->original->device_copies[dev+2] != NULL) ) {
                cost += lda * K * transfer_cost;
            }

            /* Compute the transfer cost of A */
            if ( (weight < 5) && (this_task->data._f_Au.data_in->original->device_copies[dev+2] != NULL) ) {
                cost += lda * K * transfer_cost;
            }

            /* Compute the transfer cost of B */
            if ( this_task->data._f_C.data_in->original->device_copies[dev+2] != NULL ) {
                cost += ldc * cblk_colnbr( fcblk ) * transfer_cost;
            }

	    if ( cost < mintime )
	      {
		gpuid = dev;
		mintime = cost;
	      }
        }
    }

    if ( gpuid >= 0 ) {
        pastix_atomic_inc_32b( &parsec_nbtasks_on_gpu[gpuid] );
    }

    (void)weight;
    return gpuid;
}

#else

int
zgetrf_sp1dplus_zgemm1d_selectgpu_fct( const __parsec_zgetrf_sp1dplus_GEMM1D_task_t *this_task,
                                       double weight )
{
    (void)this_task; (void)weight;
    return -2;
}

#endif /* defined(PASTIX_WITH_CUDA) */
#endif /* defined(PARSEC_GPU_WITH_CUDA) */

%}
